{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015c2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4c1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # swap the color channels, BGR to RGB\n",
    "    image = cv2.resize(image, (config.IMAGE_SIZE, config.IMAGE_SIZE)) # resize\n",
    "    image = image.astype(\"float32\") / 255.0 # scale to [0, 1]\n",
    "    \n",
    "    #transform = transforms.Compose([            \n",
    "    #    transforms.Resize(256),                    \n",
    "    #    transforms.CenterCrop(224),                \n",
    "    #    transforms.ToTensor(),                     \n",
    "    #    transforms.Normalize(                      \n",
    "    #    mean=[0.485, 0.456, 0.406],                \n",
    "    #    std=[0.229, 0.224, 0.225]                  \n",
    "    #)])\n",
    "    \n",
    "    image -= config.MEAN # scale with the mean and std in the config file\n",
    "    image /= config.STD\n",
    "    image = np.transpose(image, (2, 0, 1)) # set \"channels first\" ordering\n",
    "    image = np.expand_dims(image, 0) # add a batch dimension\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ad12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps model names to their classes\n",
    "# inside torchvision\n",
    "\n",
    "MODELS = {\n",
    "    \"vgg16\": models.vgg16(pretrained=True),\n",
    "    \"vgg19\": models.vgg19(pretrained=True),\n",
    "    \"inception\": models.inception_v3(pretrained=True),\n",
    "    \"densenet\": models.densenet121(pretrained=True),\n",
    "    \"resnet\": models.resnet50(pretrained=True)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547e604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading densenet...\n",
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n"
     ]
    }
   ],
   "source": [
    "# load our the network weights from disk, flash it to the current\n",
    "# device, and set it to evaluation mode\n",
    "model_idx = random.randint(0, len(MODELS) - 1)\n",
    "model_name = list(MODELS.keys())[model_idx]\n",
    "\n",
    "print(\"[INFO] loading {}...\".format(model_name))\n",
    "model = MODELS[model_name].to(config.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# load the image from disk, clone it (so we can draw on it later),\n",
    "# and preprocess it\n",
    "print(\"[INFO] loading image...\")\n",
    "all_images = os.listdir(\"images\")\n",
    "img_idx = random.randint(0, len(all_images) - 1)\n",
    "image = cv2.imread(os.path.join(\"images\", all_images[img_idx]))\n",
    "orig = image.copy()\n",
    "image = preprocess_image(image)\n",
    "\n",
    "# convert the preprocessed image to a torch tensor and flash it to\n",
    "# the current device\n",
    "image = torch.from_numpy(image)\n",
    "image = image.to(config.DEVICE)\n",
    "\n",
    "# load the preprocessed the ImageNet labels\n",
    "print(\"[INFO] loading ImageNet labels...\")\n",
    "imagenetLabels = dict(enumerate(open(config.IN_LABELS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a816ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image with 'densenet'...\n",
      "0. daisy: 26.96%\n",
      "1. bee: 15.33%\n",
      "2. sulphur_butterfly, sulfur_butterfly: 12.98%\n",
      "3. hummingbird: 11.99%\n",
      "4. sea_slug, nudibranch: 5.29%\n"
     ]
    }
   ],
   "source": [
    "# classify the image and extract the predictions\n",
    "print(\"[INFO] classifying image with '{}'...\".format(model_name))\n",
    "logits = model(image)\n",
    "probabilities = torch.nn.Softmax(dim=-1)(logits)\n",
    "sortedProba = torch.argsort(probabilities, dim=-1, descending=True)\n",
    "\n",
    "# loop over the predictions and display the rank-5 predictions and\n",
    "# corresponding probabilities to our terminal\n",
    "for (i, idx) in enumerate(sortedProba[0, :5]):\n",
    "    print(\"{}. {}: {:.2f}%\".format\n",
    "        (i, imagenetLabels[idx.item()].strip(),\n",
    "        probabilities[0, idx.item()] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fffe3588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw the top prediction on the image and display the image to\n",
    "# our screen\n",
    "(label, prob) = (imagenetLabels[probabilities.argmax().item()],\n",
    "    probabilities.max().item())\n",
    "\n",
    "cv2.putText(orig, \"Label: {}, {:.2f}%, predicted using {}\".format(label.strip(), prob * 100, model_name),\n",
    "    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Classification\", orig)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b177d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
